{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eabd9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import host, username, password, get_db_url\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "def get_telco_data(use_cache=True):\n",
    "# filename = 'telco_churn.csv'\n",
    "    \n",
    "    if os.path.exists('telco.csv') and use_cache:\n",
    "        print('Using cached csv')\n",
    "        return pd.read_csv('telco.csv')\n",
    "    print('Acquiring data from SQL database')\n",
    "    df = pd.read_sql('''   \n",
    "                    SELECT * \n",
    "                        FROM customers\n",
    "                        JOIN contract_types USING(contract_type_id)\n",
    "                        JOIN internet_service_types USING(internet_service_type_id)\n",
    "                        JOIN payment_types USING(payment_type_id)\n",
    "                    '''\n",
    "            , get_db_url('telco_churn'))\n",
    "    \n",
    "    df.to_csv('telco.csv', index=False)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c01db8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual modular suspects\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "# Of Mice & Machine Learning Mavericks\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text, export_graphviz\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay, mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf200664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def monthly_charges_splits(df) :   \n",
    "    if df['monthly_charges'] <= 25 :\n",
    "        return '0-25'\n",
    "    elif (df['monthly_charges'] > 25) & (df['monthly_charges'] <= 50 ):\n",
    "        return '26-50'\n",
    "    elif (df['monthly_charges'] > 50) & (df['monthly_charges'] <= 75 ):\n",
    "        return '51-75'\n",
    "    elif (df['monthly_charges'] > 75) & (df['monthly_charges'] <= 100 ):\n",
    "        return '76-100'\n",
    "    else:\n",
    "        return '>100'\n",
    "\n",
    "def total_charges_splits(df) :   \n",
    "    if df['total_charges'] <= 2000 :\n",
    "        return '0-2k'\n",
    "    elif (df['total_charges'] > 2000) & (df['total_charges'] <= 4000 ):\n",
    "        return '2k-4k'\n",
    "    elif (df['total_charges'] > 4000) & (df['total_charges'] <= 6000) :\n",
    "        return '4k-6k'\n",
    "    else:\n",
    "        return '>6k'\n",
    "    \n",
    "def tenure_splits(df) :   \n",
    "    if df['tenure'] <= 6:\n",
    "        return '1-6'\n",
    "    elif (df['tenure'] > 6) & (df['tenure'] <= 12 ):\n",
    "        return '7-12'\n",
    "    elif (df['tenure'] > 12) & (df['tenure'] <= 18) :\n",
    "        return '13-18'\n",
    "    elif df['tenure'] > 18 & (df['tenure'] <= 24) :\n",
    "        return '19-24'\n",
    "    else:\n",
    "        return '>24'\n",
    "\n",
    "\n",
    "\n",
    "def split_telco_data(df):\n",
    "    '''\n",
    "    This function performs split on telco data, stratify churn.\n",
    "    Returns train, validate, and test dfs.\n",
    "    '''\n",
    "    # the initial 80/20 split. the test set constitutes 20% of the original df.\n",
    "    train_validate, test = train_test_split(df, test_size=.2, \n",
    "                                        random_state=123, \n",
    "                                        stratify=df.churn_encoded)\n",
    "    \n",
    "    # the subsequent 70/30 split. For the remaining 80%, .7 goes to train and .3 to validate\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                   random_state=123, \n",
    "                                   stratify=train_validate.churn_encoded)\n",
    "    return train, validate, test\n",
    "\n",
    "def prep_telco_data(df):\n",
    "    # Drop duplicate columns\n",
    "    df.drop(columns=['payment_type_id', 'internet_service_type_id', 'contract_type_id'], inplace=True)\n",
    "       \n",
    "    # Drop null values stored as whitespace    \n",
    "    df['total_charges'] = df['total_charges'].str.strip()\n",
    "    df = df[df.total_charges != '']\n",
    "    \n",
    "    # Convert to correct datatype\n",
    "    df['total_charges'] = df.total_charges.astype(float)\n",
    "\n",
    "    # Convert binary categorical variables to numeric\n",
    "    df['gender_encoded'] = df.gender.map({'Female': 1, 'Male': 0})\n",
    "    df['partner_encoded'] = df.partner.map({'Yes': 1, 'No': 0})\n",
    "    df['dependents_encoded'] = df.dependents.map({'Yes': 1, 'No': 0})\n",
    "    df['phone_service_encoded'] = df.phone_service.map({'Yes': 1, 'No': 0})\n",
    "    df['paperless_billing_encoded'] = df.paperless_billing.map({'Yes': 1, 'No': 0})\n",
    "    df['churn_encoded'] = df.churn.map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Get dummies for non-binary categorical variables\n",
    "    dummy_df = pd.get_dummies(df[['multiple_lines', \\\n",
    "                              'online_security', \\\n",
    "                              'online_backup', \\\n",
    "                              'device_protection', \\\n",
    "                              'tech_support', \\\n",
    "                              'streaming_tv', \\\n",
    "                              'streaming_movies', \\\n",
    "                              'contract_type', \\\n",
    "                              'internet_service_type', \\\n",
    "                              'payment_type']], dummy_na=False, \\\n",
    "                              drop_first=False)\n",
    "    \n",
    "    # Concatenate dummy dataframe to original \n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "               \n",
    "    # create categorical columns for these numerical fields\n",
    "    df['monthlycharges_group'] = df.apply(lambda df:monthly_charges_splits(df), axis = 1)\n",
    "    df['totalcharges_group'] = df.apply(lambda df:total_charges_splits(df), axis = 1)\n",
    "    df['tenure_months'] = df.apply(lambda df:tenure_splits(df), axis = 1)\n",
    "    \n",
    "    # now I can divvy up these groups of charges and be on my way to scaling. \n",
    "    numerical_numbskulls = ['monthlycharges_group','totalcharges_group','tenure_months']\n",
    "    for col in numerical_numbskulls:\n",
    "        dummy_df = pd.get_dummies(df[col],\n",
    "                                  prefix=f'enc_{df[col].name}',\n",
    "                                  drop_first=False,\n",
    "                                  dummy_na=False)        \n",
    "        # add the columns to the dataframe\n",
    "        df = pd.concat([df, dummy_df], axis=1)\n",
    "    df = df.drop(columns=numerical_numbskulls)\n",
    "    \n",
    "    num_cols = ['monthly_charges', 'total_charges']\n",
    "\n",
    "    #Scaling Numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(df[num_cols])\n",
    "    scaled = pd.DataFrame(scaled,columns=num_cols)\n",
    "    \n",
    "    #dropping original values merging scaled values for numerical columns\n",
    "    df = df.drop(columns = num_cols, axis = 1)\n",
    "    df = df.merge(scaled, left_index=True, right_index=True, how = \"left\")\n",
    "    \n",
    "        # Selective dummy removal since drop_first left unwanted cols and removed favored\n",
    "    # kept it mostly random as far as selection of what to drop goes. Purposefully\n",
    "    # excluded no internet service when optional. \n",
    "    df = df.drop(columns=['contract_type_One year', 'device_protection_No internet service',\n",
    "                          'enc_monthlycharges_group_0-25', 'enc_tenure_months_19-24',\n",
    "                          'enc_totalcharges_group_4k-6k', 'internet_service_type_DSL',\n",
    "                          'multiple_lines_No phone service','online_backup_No internet service',\n",
    "                          'online_security_No internet service', 'payment_type_Mailed check',\n",
    "                          'streaming_movies_No internet service', 'streaming_tv_No internet service',\n",
    "                          'tech_support_No internet service'\n",
    "                         ]\n",
    "                )\n",
    "                   \n",
    "    df = df.rename(columns={'internet_service_type_DSL': 'dsl',\n",
    "                                   'internet_service_type_Fiber optic': 'fiber_optic',\n",
    "                                   'internet_service_type_None': 'no_internet',\n",
    "                                   'contract_type_Month-to-month': 'monthly',\n",
    "                                   'contract_type_Two year': 'two_year_contract',\n",
    "                                   'payment_type_Bank transfer (automatic)': 'auto_bank_transfer',\n",
    "                                   'payment_type_Credit card (automatic)': 'auto_credit_card',\n",
    "                                   'payment_type_Electronic check': 'electronic_check'\n",
    "                            }\n",
    "                   )\n",
    "    \n",
    "    \n",
    "    # split the data\n",
    "    train, validate, test = split_telco_data(df)\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12e3cd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached csv\n"
     ]
    }
   ],
   "source": [
    "df = get_telco_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a00917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qn/sd1_3x2j4196db03067vql600000gn/T/ipykernel_14744/3498182918.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['total_charges'] = df.total_charges.astype(float)\n",
      "/var/folders/qn/sd1_3x2j4196db03067vql600000gn/T/ipykernel_14744/3498182918.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['gender_encoded'] = df.gender.map({'Female': 1, 'Male': 0})\n",
      "/var/folders/qn/sd1_3x2j4196db03067vql600000gn/T/ipykernel_14744/3498182918.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['partner_encoded'] = df.partner.map({'Yes': 1, 'No': 0})\n",
      "/var/folders/qn/sd1_3x2j4196db03067vql600000gn/T/ipykernel_14744/3498182918.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['dependents_encoded'] = df.dependents.map({'Yes': 1, 'No': 0})\n",
      "/var/folders/qn/sd1_3x2j4196db03067vql600000gn/T/ipykernel_14744/3498182918.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['phone_service_encoded'] = df.phone_service.map({'Yes': 1, 'No': 0})\n",
      "/var/folders/qn/sd1_3x2j4196db03067vql600000gn/T/ipykernel_14744/3498182918.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['paperless_billing_encoded'] = df.paperless_billing.map({'Yes': 1, 'No': 0})\n",
      "/var/folders/qn/sd1_3x2j4196db03067vql600000gn/T/ipykernel_14744/3498182918.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['churn_encoded'] = df.churn.map({'Yes': 1, 'No': 0})\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = prep_telco_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f9230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'monthly', 'two_year_contract', 'fiber_optic', 'electronic_check',\n",
    "    'enc_tenure_months_1-6', 'enc_tenure_months_7-12', 'enc_tenure_months_13-18',\n",
    "    'auto_credit_card', 'auto_bank_transfer', 'no_internet',\n",
    "    'online_security_No', 'online_security_Yes', 'online_backup_No', 'online_backup_Yes',\n",
    "    'device_protection_No', 'device_protection_Yes', 'tech_support_No', 'tech_support_Yes'   \n",
    "]\n",
    "\n",
    "X_train, y_train = train[features], train.churn_encoded\n",
    "X_validate, y_validate = validate[features], validate.churn_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a59966b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(X, y, strategy='most_frequent', random_state=123):\n",
    "    '''\n",
    "    Generates a baseline model using sklearn DummyClassifier strategy\n",
    "    default set to 'most_frequent' and random_state=123\n",
    "    '''\n",
    "\n",
    "    # assign baseline model and fit to data\n",
    "    baseline = DummyClassifier(strategy=strategy, random_state=random_state)\n",
    "    baseline.fit(X, y)\n",
    "    # assign baseline predictions\n",
    "    y_baseline = baseline.predict(X)\n",
    "    # print baseline accuracy score and first ten values for training data\n",
    "    print(f'''\n",
    "               Baseline Accuracy Score: {baseline.score(X, y):.2%}\n",
    "        First Ten Baseline Predictions: {y_baseline[:10]}\n",
    "        ''')\n",
    "\n",
    "    return baseline, y_baseline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33655c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               Baseline Accuracy Score: 73.43%\n",
      "        First Ten Baseline Predictions: [0 0 0 0 0 0 0 0 0 0]\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "baseline, baseline_pred = baseline_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9171d6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy:  0.7938877043354655\n"
     ]
    }
   ],
   "source": [
    "# Final Test\n",
    "\n",
    "# re-creating the model using the given features and hyperparameters\n",
    "\n",
    "# separate each sample into x (features) and y (target)\n",
    "x_train = train[features]\n",
    "y_train = train.churn_encoded\n",
    "\n",
    "x_test = test[features]\n",
    "y_test = test.churn_encoded\n",
    "\n",
    "# create the classifier\n",
    "\n",
    "# Best working KNN\n",
    "clf = DecisionTreeClassifier(criterion = 'entropy',\n",
    "                              max_depth = 6, min_samples_leaf = 1,\n",
    "                              min_samples_split = 30)\n",
    "\n",
    "\n",
    "# fit the classifier to the training data\n",
    "clf = clf.fit(x_train, y_train)\n",
    "\n",
    "# create predictions for the model's performance on the test set\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# establish the probability for those predictions created above\n",
    "y_pred_proba = clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "print('model accuracy: ', accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65b3cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the appropriate columns\n",
    "predictions = pd.DataFrame(columns=['customer_id', 'churn_probability', 'churn_prediction'])\n",
    "# set customer id using the customer_id column from the original database\n",
    "predictions['customer_id'] = test.customer_id\n",
    "# set the churn_probability column using the probabilities created above\n",
    "predictions['churn_probability'] = y_pred_proba\n",
    "# set the predictions column using the predictions created above\n",
    "predictions['churn_prediction'] = y_pred\n",
    "# write to a local csv file\n",
    "predictions.to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718c756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
